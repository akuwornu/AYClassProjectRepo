---
title: "AY_ClassProject_FramingAI"
author: "AY Kuwornu"
date: "2024-11-12"
output: html_document
---

##Project Title: Analyzing AI & Disaster Framing through Organizational Press Releases

##Summary of Project:
Our project seeks to analyze how U.S. organizations frame AI in press releases about disaster response and management. We plan to focus on how  AI is framed in organizational press releases on crisis and disaster management, the key themes and dominant narratives related the discussions of AI in the context of natural disasters and crises in United States and finally what potential risks and ethical concerns regarding AI usage are highlighted in these communications?

##Link to my github repository: 
"https://github.com/akuwornu/AYClassProjectRepo"

##Link to data:
https://github.com/akuwornu/AYClassProjectRepo/blob/main/AY_framingAI.PDF

#Describe your data source

Data collected from Nexis Uni, especially focusing on the New Wires and press releases from PR Newswire, Business Wire etc. 

#Spell out your content analysis plan, including preliminary work on a code book

#Objective of the Project: 
 
Analyze how U.S. organizations frame AI in press releases concerning disaster response and management to draw out relevant themes and trends

#Research Questions:

  1. How is AI framed in organizational press releases related to disaster management?
  2. What themes and narratives dominate discussions of AI in the context of natural disasters and crises?
  3. What potential risks and ethical concerns regarding AI usage are highlighted in these communications?
  
#Data collected:

Press Releases collected using Nexis Uni, from Nov. 1, 2023, to Nov.1, 2024. 
Search criteria included the following keywords: 'Artificial Intelligence' "OR" 'AI' "AND" 'disasters', 'emergencies', or 'crisis management'.


#Preliminary work on a code book is - In the Codebook Structure, I would define; 

Code Name - Definition	- Examples -	Coding Rules

#few tentative Code Names
AI Framing
Ethical Concerns
Misinformation Risks
Public Trust
AI Benefits
AI Limitations
Disaster Types
Organizational Strategies


#Provide a sample of the data and some descriptive statistics.

Total Press Releases:       	500
Time Period Covered:        	Nov. 1, 2023 â€“ Nov. 1, 2024
Average Releases Per Month: 	125
Top Mentioned Keywords:       "Artificial Intelligence," "AI," "Disaster"
Dominant headlines of the press release:   Ethical Concerns, Public Trust, Misinformation, AI Benefits, AI Limitations
Most Active Month:	July 2024
Least Active Month:	February 2024
Regional Focus:	United States
Organizations Highlighted:	FEMA, Homeland Security, Tech Companies, NGOs



#Load the appropriate software libraries

```{r echo= T, results = 'hide', warning=F, error=F, message=F}
library(tidyverse)
library(pdftools)
library(tidyverse)
library(textdata)
library(tidytext)
library(quanteda)
library(rio)
library(janitor)
library(rio)
library(dplyr)
library(stringr)
library(readr)
here::here()
# from tutorial packages
library(DT)
library(knitr) 
library(kableExtra) 
library(ggplot2)
library(SnowballC)
library(flextable)
```

#I am Loading the data here from the folder, while reading the files, combining the text, spliting into the document. 

```{r}
#reading PDF file and converting it into text. 
text <- pdf_text("../AYClassProjectRepo/AY_framingAI.PDF")
writeLines(text, "../AYClassProjectRepo/AY_framingAI.text")
file_path <- "../AYClassProjectRepo/AY_framingAI.text"
text_data <- readLines(file_path)
text_combined <- paste(text_data, collapse = "\n")
documents <- strsplit(text_combined, "End of Document")[[1]]
output_dir <- "./extracted files"
for (i in seq_along(documents)) {
  output_file <- file.path(output_dir, paste0("AIframes_extracted", i, ".txt"))
  writeLines(documents[[i]], output_file) 
}
cat("Files created:", length(documents), "\n")
FramingAI_index <- read_lines("./extracted files/AIframes_extracted1.txt")
extracted_lines <-FramingAI_index[20:2030]
cat(head(extracted_lines, 20), sep = "\n")
extracted_lines <- extracted_lines |> 
  as.data.frame() 
```

## Build a final dataframe index

```{r}

cleaned_data <- extracted_lines |>
  mutate(
    trimmed_line = str_trim(extracted_lines),  
    is_title = str_detect(trimmed_line, "^\\d+\\. "),  

    # Detect dates (e.g., "Aug 14, 2024")
    is_date = str_detect(trimmed_line, "\\b\\w{3} \\d{1,2}, \\d{4}\\b")
  )

# Shifting dates to align with corresponding titles
aligned_data <- cleaned_data |>
  mutate(
    date = ifelse(lead(is_date, 1), lead(trimmed_line, 1), NA_character_)  # Shift date to title's row
  ) |>
  filter(is_title) |>
  select(trimmed_line, date)  # Keep only the relevant columns

# Rename columns for clarity
final_data <- aligned_data |>
  rename(
    title = trimmed_line,
    date = date
  )

# Date and Publication in separate columns, and formatted
final_data <- separate(data = final_data, col = date, into = c("date2", "publication"), sep = "  ", extra = "merge", fill = "right")


# Format date, clean headline
final_data <- final_data |> 
  mutate(date = as.Date(date2,format = "%b %d, %Y")) |> 
  mutate(title =str_remove(title, "^\\d+\\. ")) |> 
  subset(select = -(date2)) |> 
  mutate(index = row_number()) |> 
  select(index, date, title, publication)

write_csv(final_data, "../AYClassProjectRepo/final_data.csv")
```

## Raw text compiler 
###created an index with the file path to the stories. And then compiled the stories into a dataframe

```{r include=FALSE}

files <- list.files("../AYClassProjectRepo/extracted files", pattern="*.txt") %>% 
  as.data.frame() |> 
  rename(filename = 1) |> 
  
  #create an index with the file name
 mutate(index = str_extract(filename, "\\d+")) |> 
  mutate(index = as.numeric(index))

#Join the file list to the index

final_index <- final_data |> 
  inner_join(files, c("index")) |> 
mutate(filepath = paste0("../AYClassProjectRepo/extracted files/", filename))

head(final_index)

```

## Text compiler
```{r include=FALSE}

# Define function to loop through each text file 

create_article_text <- function(row_value) {
  temp <- final_index %>%
    slice(row_value)
  
 
  temp_filename <- temp$filename
  
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
 
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

articles_df <- tibble()
row_values <- 1:nrow(final_index)

lapply(row_values, create_article_text)


# Clean up articles_df and join to index dataframe
articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(final_index)

#After viewing articles_df, I see 2022 lines from the index that I don't need. Cutting them 

articles_df <- articles_df %>%
  slice(-c(1:2022)) |> 
  #gets rid of blank rows
    filter(trimws(sentence) != "") 

write_csv(articles_df, "../AYClassProjectRepo/extracted files/articles_df.csv")
```

```{r}

# Load required libraries
library(dplyr)
library(stringr)
library(tidytext)

# Ensure `stop_words` data is available
data("stop_words")

# Define the expanded remove pattern for unwanted terms
remove_pattern <- paste(
  "title|pages|publication date|publication subject|publication type|issn|language of publication: english|",
  "document url|copyright|last updated|database|startofarticle|proquest document id|",
  "classification|https|--|people|alt|language|english|length|words|publication|type|morg|york|times|'new york times'|",
  "publication info|illustration|date|caption|[0-9.]|new york times|alexandria virgina|ms jean|rights reserved|indo pific|",
  "identifier/keyword|twitter\\.|rauchway|keynes's|_ftn|enwikipediaorg|",
  "wwwnytimescom|wwwoenbat|wwwpresidencyucsbedu|wwwalt|wwwthemoneyillusioncom|",
  "aaa|predated|a_woman_to_reckon_with_the_vision_and_legacy_of_fran|ab_se|jcr:fec|ac|\\bwww\\b|[_]+",
  sep = ""
)


# Process bigrams
bigrams <- articles_df %>% 
  select(sentence) %>% 
  mutate(
    sentence = str_squish(sentence),                      # Remove extra spaces
    sentence = tolower(sentence),                         # Convert to lowercase
    sentence = str_replace_all(sentence, remove_pattern, ""), # Remove unwanted terms
    sentence = str_replace_all(sentence, "- ", ""),       # Remove trailing hyphens
    sentence = str_replace_all(sentence, "\\b[a-zA-Z]\\b", "") # Remove single characters
  ) %>% 
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%                 # Filter out stop words
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE) %>%                   # Count bigrams
  filter(!is.na(word1) & !is.na(word2))                   # Filter out NAs

bigrams
```


```{r}
nrow(final_data)
ncol(final_data)
```

#top 20 bigrams
```{r}
top_20_bigrams <- bigrams |> 
   top_n(20) |> 
  mutate(bigram = paste(word1, " ", word2)) |> 
  select(bigram, n)
```


```{r}
ggplot(top_20_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  coord_flip() +  
  labs(title = "Top Two-Word phrases in FramingAI articles",
       caption = "n=500 articles. Graphic by AY Kuwornu. 11-30-2024",
       x = "Phrase",
       y = "Count of terms")
```
#Sentiment Analysis

```{r}
library(textdata)
library(quanteda)
```

#Tokenize text

```{r}
text_tokenized <- articles_df %>% 
  select(sentence) %>% 
  unnest_tokens(word, sentence)
```

#Filter tokenized text
```{r}
text_tokenized <- articles_df %>% 
  select(sentence) %>% 
  mutate(sentence = str_replace_all(sentence, "- ", "")) %>% 
  unnest_tokens(word, sentence) %>% 
  filter(!word %in% stop_words$word) %>% 
  filter(word != "stories_corpus") %>%
  filter(!grepl('[0-9]', word))
```


#Do a word count
```{r}
text_word_ct <- text_tokenized %>%
  count(word, sort=TRUE)
text_word_ct
```


#Load NRC lexicon
```{r}
nrc_sentiments <- get_sentiments("nrc")

nrc_sentiments %>% count(sentiment)

nrc_sentiments %>% 
  group_by(word) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  distinct()
```

# Count Overall Sentiment with NRC
```{r}

sentiments_all <- text_tokenized %>%
  inner_join(nrc_sentiments) 

sentiments_all %>% 
  group_by(word) %>% 
    count(sentiment) %>% 
  arrange(desc(n))
```

```{r}
sentiments_all <- text_tokenized %>%
  inner_join(nrc_sentiments) %>%
  count(sentiment, sort = TRUE) %>% 
  mutate(pct_total =round(n/sum(n), digits=2))

sentiments_all
```

# Create ggplot
```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Plot the sentiment analysis results
nrc_plot <- sentiments_all %>% 
  ggplot(aes(x = reorder(sentiment, n), y = n, fill = sentiment)) + 
  geom_bar(stat = "identity", position = "dodge") +  
  theme(legend.position = "none") +  
  labs(
    title = "Sentiments reflected in Articles About AI & Disaster",
    subtitle = "positive & negative sentiments recorded ",
    caption = "NRC Sentiment Analysis. Graphic by AY Kuwornu, Dec 1, 2024",
    x = "Total Sentiment Score",
    y = "Score"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.caption = element_text(hjust = 0, size = 10)
  ) +
  scale_fill_brewer(palette = "Set3") +
  coord_flip()  
# Print the plot
print(nrc_plot)
```

<<<<<<< HEAD

#Check the words in some of the sentiments

#Positive
```{r}
nrc_positive <- nrc_sentiments %>%
  filter(sentiment == "positive")
FramingAI_positive <- text_tokenized %>%
  inner_join(nrc_positive) %>%
  count(word, sort = TRUE)
FramingAI_positive
```

#Trust
```{r}
#Trust
nrc_trust <- nrc_sentiments %>%
  filter(sentiment == "trust")
FramingAI_trust <- text_tokenized %>%
  inner_join(nrc_trust) %>%
  count(word, sort = TRUE)
FramingAI_trust
```


#Negative
```{r}
nrc_negative <- nrc_sentiments %>%
  filter(sentiment == "negative")
FramingAI_negative <- text_tokenized %>%
  inner_join(nrc_negative) %>%
  count(word, sort = TRUE)
FramingAI_negative
```


#Fear
```{r}
nrc_fear <- nrc_sentiments %>%
  filter(sentiment == "fear")
FramingAI_fear <- text_tokenized %>%
  inner_join(nrc_fear) %>%
  count(word, sort = TRUE)
FramingAI_fear
```


#Anticipation
```{r}
nrc_anticipation <- nrc_sentiments %>%
  filter(sentiment == "anticipation")
FramingAI_anticipation <- text_tokenized %>%
  inner_join(nrc_anticipation) %>%
  count(word, sort = TRUE)
FramingAI_anticipation
```

#Anger
```{r}
nrc_anger <- nrc_sentiments %>%
  filter(sentiment == "anger")
FramingAI_anger <- text_tokenized %>%
  inner_join(nrc_anger) %>%
  count(word, sort = TRUE)
FramingAI_anger
```

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)

# Define sentiments to analyze
sentiments <- c("positive", "trust", "negative", "fear", "anticipation", "anger")

# Process each sentiment and combine results into a single table
final_table <- lapply(sentiments, function(sentiment) {
  nrc_sentiments %>%
    filter(sentiment == !!sentiment) %>%
    inner_join(text_tokenized, by = "word") %>%
    count(word, sort = TRUE) %>%
    mutate(sentiment = sentiment)
}) %>%
  bind_rows() %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)

# View final table
print(final_table)

```


#In this part, i used the excel data downloaded from NexisUni to generate some useful statistics

#Create ggplot chart showing the distribution of the data over time

#Import the excel file
```{r}
library(tidyverse)
AI_disaster  <- rio::import("../AYClassProjectRepo/AY_framingAI.XLSX") %>%
  janitor::clean_names()
head(AI_disaster)
```


#I am presenting ggplot based on per week data. 

```{r}
library(tidyverse)
library(janitor)
library(lubridate)

# Clean and preprocess the data
AI_ggplot <- AI_disaster |> 
  janitor::clean_names() |> 
  mutate(published_date_bak = published_date) |> 
  separate(published_date, into = c("date", "year_day"), sep = ", ", extra = "merge", fill = "right") |> 
  mutate(date = as.Date(date, format = "%B %d")) |> # Convert to proper date format
  mutate(week = floor_date(date, unit = "week", week_start = 1)) |> # Group by week (starting on Monday)
  group_by(week) |> 
  count(week)

# Plotting the data
ggplot(AI_ggplot, aes(x = week, y = n)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color = "red", size = 2) +
  scale_x_date(date_labels = "%b %d, %Y", date_breaks = "1 week") + # Show labels for every week
  labs(
    title = "Count of Press Releases Over Time (Weekly)",
    caption = "Weekly data depiction.  Graphic by AY Kuwornu, Dec 1, 2024",
    x = "Published Week",
    y = "Number of Press Releases"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.major = element_line(size = 0.5, color = "gray"),
    panel.grid.minor = element_blank()
  )


```

#Where most Press Release published 
```{r}
pr_published_by <- AI_disaster %>%
  count(publication_4) %>%  
  group_by(publication_4) %>%  
  summarise(pr_published_by = sum(n)) %>%  
  arrange(desc(pr_published_by)) %>% 
 head(10)
```


#Visualize top 10
```{r}
# Assuming `pr_published_by` is already created as per your code
library(ggplot2)
library(ggthemes) # For additional themes

# Create a better-looking bar chart
ggplot(pr_published_by, aes(x = reorder(publication_4, -pr_published_by), y = pr_published_by, fill = pr_published_by)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_text(aes(label = pr_published_by), vjust = -0.5, size = 4, color = "black") +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  theme_minimal(base_size = 12) +
  labs(
    title = "Top 10 Publications on AI and Disaster",
    subtitle = "Number of press releases published by each publication",
    caption = "location of most published press releases. Graphic by AY Kuwornu, Dec 1, 2024",
    x = "Publication",
    y = "Count of Published PRs"
  ) +
  theme(
    plot.title = element_text(face = "bold", size = 15, hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    axis.text.x = element_text(angle = 30, hjust = 1, size = 9),
    panel.grid.major = element_line(color = "grey80", linetype = "dashed")
  )

```



=======
>>>>>>> parent of 7ec0966 (updated)
##Using topic modeling, I examined the idea of topics reflected in our data. 

```{r}
# install.packages("here")
# install.packages("tidytext")
# install.packages("quanteda")
# install.packages("tm")
# install.packages("topicmodels")
# install.packages("reshape2")
# install.packages("ggplot2")
# install.packages("wordcloud")
# install.packages("pals")
# install.packages("SnowballC")
# install.packages("lda")
# install.packages("ldatuning")
# install.packages("kableExtra")
# install.packages("DT")
# install.packages("flextable")
# install.packages("remotes")
# remotes::install_github("rlesur/klippy")
#install.packages("rio")
#install.packages("readtext")
#install.packages("formattable")

```

#Loading relevant libraries and packages for topic modelling

```{r include=FALSE}
# set options
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation
# load packages
here::here()
library(tidyverse)
library(tidytext)
library(rio)
library(readtext)
#topic modeling
library(quanteda)
library(tm)
library(topicmodels)
library(lda)
library(ldatuning)
# from tutorial packages
library(DT)
library(knitr) 
library(kableExtra) 
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(flextable)

```

### Import Data and process into corpus

```{r}
topic_data <- articles_df %>% 
  select(filename, sentence) %>% 
  as.data.frame() %>% 
  rename(doc_id = filename, text= sentence)

# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# create corpus object
corpus <- Corpus(DataframeSource(topic_data))
# Preprocessing chain
processedCorpus <- tm_map(corpus, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)

```


```{r tm3a}
#DTM: rows correspond to the documents in the corpus. Columns correspond to the terms in the documents. Cells correspond to the weights of the terms. (Girder)
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
topic_data <- topic_data[sel_idx, ]
#5 term minimum[1] 1387 3019
#5 term minimum[1] 308597 10339

```

```{r tm12}
# number of topics
# K <- 20
K <- 6
# set random number generator seed
set.seed(9161)
#Latent Dirichlet Allocation, LDA
topicModel2 <- LDA(DTM, K, method="Gibbs", control=list(iter = 1000, verbose = 25, alpha = 0.2))
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel2, 10), 2, paste, collapse = " ")  # reset topicnames

```

### Extracting and visualizing the topics

```{r}
# Step 1: Check dimensions
n_theta <- nrow(theta)
n_topicdata<- length(topic_data)

cat("Number of rows in theta: ", n_theta, "\n")
cat("Number of documents in textdata: ", n_topicdata, "\n")

# Check if textdata contains all the documents in theta
common_ids <- intersect(rownames(theta), topic_data$doc_id) # Assuming textdata has a 'doc_id' column

# Filter textdata to include only the documents present in theta
topicdata_filtered <- topic_data[topic_data$doc_id %in% common_ids, ]

# Check dimensions after filtering
n_topicdata_filtered <- nrow(topicdata_filtered)
cat("Number of documents in filtered textdata: ", n_topicdata_filtered, "\n")


# Align rownames of theta with filtered textdata
theta_aligned <- theta[rownames(theta) %in% topicdata_filtered$doc_id, ]

# Step 2: Combine data
full_data <- data.frame(theta_aligned, decade = topicdata_filtered)

# get mean topic proportions per decade
# topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)
# set topic names to aggregated columns
colnames(full_data)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(full_data)
   
#Examine topic names
#enframe(): Converts a named list into a dataframe.
topics <- enframe(topicNames, name = "number", value = "text") %>% 
  unnest(cols = c(text)) 
  
topics
```
# Review the topics and determine a 1-2 word label after reading the source documents.

```{r}
theta2 <- as.data.frame(theta)

topic1 <- theta2 %>% 
  rownames_to_column(var = "file") |> # putting the rownames into a new column called file
  mutate(file = str_remove(file, "^X"),  # Remove leading 'X'
         line = str_extract(file, "(?<=\\.txt)\\.\\d+")) |>   # Extract number after .txt
  mutate(file = str_remove(file, "\\.\\d+$")) |> 
  rename(topic1 = '1') |> 
  top_n(20, topic1) |> 
  arrange(desc(topic1)) |>  
  select(file, line, topic1)

```

```{r}
vizDataFrame <- vizDataFrame %>% 
  mutate(category = case_when(
    str_detect(variable, "data technolog servic manag provid system market cloud intellig comput") ~ "managing crisis",
    str_detect(variable, "servic program communiti develop support health energi news includ climat") ~ "community engagement",
    str_detect(variable, "committe depart state hous year issu act fund offic report") ~ "government involvement",
    str_detect(variable, "state secur unit support cooper region continu countri forc commit") ~ "approach",
    str_detect(variable, "year result oper million financi quarter net compani increas includ") ~ "outcome",
    str_detect(variable, "page presid census time peopl word length load-dat bureau american
") ~ "reflection",
    TRUE ~ NA_character_  # Ensure there is a default value for non-matching cases
  ))
```

```
```
## 800 word analysis

Following my goal to analyze how U.S. organizations frame AI in press releases concerning disaster response and management to draw out relevant themes and trends, I run codes to try to answer the initial research questions, which were:
  1. How is AI framed in organizational press releases related to disaster management?
  2. What themes and narratives dominate discussions of AI in the context of natural disasters and crises?
  3. What potential risks and ethical concerns regarding AI usage are highlighted in these communications?
For a better understanding of the data, I attempted to answer the questions with evidence from the data. In this section, I will highlight each research question and provide some insights into the answers as derived from the data.

#How is AI framed in organizational press releases related to disaster management?
Here, I conducted sentiment analysis to gauge the sentiments of the key words used in most of the data collected. Using the NRC dictionary, it appeared most organizational press releases studied framed AI in both positive and negative ways. A breakdown of the words showed the massive use of words such as U.S, security, data, ai, including, etc. suggesting majority of the press releases came from government agencies such as the national security, government departments, among others. This means most of the press releases analyzed were from government organizations mainly based on natural disasters like pandemics, hurricanes, etc. But the sentiment analysis of the bigrams from these press releases showed mostly positive sentiments for words like disaster, defence, security and ai. 
