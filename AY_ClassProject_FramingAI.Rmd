---
title: "AY_ClassProject_FramingAI"
author: "AY Kuwornu"
date: "2024-11-12"
output: html_document
---

##Project Title: Analyzing AI & Disaster Framing through Organizational Press Releases

##Summary of Project:
Our project seeks to analyze how U.S. organizations frame AI in press releases about disaster response and management. We plan to focus on how  AI is framed in organizational press releases on crisis and disaster management, the key themes and dominant narratives related the discussions of AI in the context of natural disasters and crises in United States and finally what potential risks and ethical concerns regarding AI usage are highlighted in these communications?

##Link to my github repository: 
"https://github.com/akuwornu/AYClassProjectRepo"

##Link to data:
https://github.com/akuwornu/AYClassProjectRepo/blob/main/AY_framingAI.PDF

#Describe your data source

Data collected from Nexis Uni, especially focusing on the New Wires and press releases from PR Newswire, Business Wire etc. 

#Spell out your content analysis plan, including preliminary work on a code book

#Objective of the Project: 
 
Analyze how U.S. organizations frame AI in press releases concerning disaster response and management.

#Research Questions:

  1. How is AI framed in organizational press releases related to disaster management?
  2. What themes and narratives dominate discussions of AI in the context of natural disasters and crises?
  3. What potential risks and ethical concerns regarding AI usage are highlighted in these communications?
  
#Data collected:

Press Releases collected using Nexis Uni, from Nov. 1, 2023, to Nov.1, 2024. 
Search criteria included the following keywords: 'Artificial Intelligence' "OR" 'AI' "AND" 'disasters', 'emergencies', or 'crisis management'.


#Preliminary work on a code book is - In the Codebook Structure, I would define; 

Code Name - Definition	- Examples -	Coding Rules

#few tentative Code Names
AI Framing
Ethical Concerns
Misinformation Risks
Public Trust
AI Benefits
AI Limitations
Disaster Types
Organizational Strategies


#Provide a sample of the data and some descriptive statistics.

Total Press Releases:       	500
Time Period Covered:        	Nov. 1, 2023 â€“ Nov. 1, 2024
Average Releases Per Month: 	125
Top Mentioned Keywords:       "Artificial Intelligence," "AI," "Disaster"
Dominant headlines of the press release:   Ethical Concerns, Public Trust, Misinformation, AI Benefits, AI Limitations
Most Active Month:	July 2024
Least Active Month:	February 2024
Regional Focus:	United States
Organizations Highlighted:	FEMA, Homeland Security, Tech Companies, NGOs



#Load the appropriate software libraries

```{r echo= T, results = 'hide', warning=F, error=F, message=F}
library(tidyverse)
library(pdftools)
library(tidyverse)
library(textdata)
library(tidytext)
library(quanteda)
library(rio)
library(janitor)
library(rio)
library(dplyr)
library(stringr)
library(readr)
here::here()
# from tutorial packages
library(DT)
library(knitr) 
library(kableExtra) 
library(ggplot2)
library(SnowballC)
library(flextable)
```

#I am Loading the data here from the folder, while reading the files, combining the text, spliting into the document. 

```{r}
#reading PDF file and converting it into text. 
text <- pdf_text("https://github.com/akuwornu/AYClassProjectRepo/blob/main/AY_framingAI.PDF")
writeLines(text, "../AYClassProjectRepo/AY_framingAI.text")
file_path <- "../AYClassProjectRepo/AY_framingAI.text"
text_data <- readLines(file_path)
text_combined <- paste(text_data, collapse = "\n")
documents <- strsplit(text_combined, "End of Document")[[1]]
output_dir <- "./extracted files"
for (i in seq_along(documents)) {
  output_file <- file.path(output_dir, paste0("AIframes_extracted", i, ".txt"))
  writeLines(documents[[i]], output_file) 
}
cat("Files created:", length(documents), "\n")
FramingAI_index <- read_lines("./extracted files/AIframes_extracted1.txt")
extracted_lines <-FramingAI_index[20:2030]
cat(head(extracted_lines, 20), sep = "\n")
extracted_lines <- extracted_lines |> 
  as.data.frame() 
```

## Build a final dataframe index

```{r}

cleaned_data <- extracted_lines |>
  mutate(
    trimmed_line = str_trim(extracted_lines),  
    is_title = str_detect(trimmed_line, "^\\d+\\. "),  

    # Detect dates (e.g., "Aug 14, 2024")
    is_date = str_detect(trimmed_line, "\\b\\w{3} \\d{1,2}, \\d{4}\\b")
  )

# Shifting dates to align with corresponding titles
aligned_data <- cleaned_data |>
  mutate(
    date = ifelse(lead(is_date, 1), lead(trimmed_line, 1), NA_character_)  # Shift date to title's row
  ) |>
  filter(is_title) |>
  select(trimmed_line, date)  # Keep only the relevant columns

# Rename columns for clarity
final_data <- aligned_data |>
  rename(
    title = trimmed_line,
    date = date
  )

# Date and Publication in separate columns, and formatted
final_data <- separate(data = final_data, col = date, into = c("date2", "publication"), sep = "  ", extra = "merge", fill = "right")


# Format date, clean headline
final_data <- final_data |> 
  mutate(date = as.Date(date2,format = "%b %d, %Y")) |> 
  mutate(title =str_remove(title, "^\\d+\\. ")) |> 
  subset(select = -(date2)) |> 
  mutate(index = row_number()) |> 
  select(index, date, title, publication)

write_csv(final_data, "../AYClassProjectRepo/final_data.csv")
```

## Raw text compiler 
###created an index with the file path to the stories. And then compiled the stories into a dataframe

```{r include=FALSE}

files <- list.files("../AYClassProjectRepo/extracted files", pattern="*.txt") %>% 
  as.data.frame() |> 
  rename(filename = 1) |> 
  
  #create an index with the file name
 mutate(index = str_extract(filename, "\\d+")) |> 
  mutate(index = as.numeric(index))

#Join the file list to the index

final_index <- final_data |> 
  inner_join(files, c("index")) |> 
mutate(filepath = paste0("../AYClassProjectRepo/extracted files/", filename))

head(final_index)

```

## Text compiler
```{r}

# Define function to loop through each text file 

create_article_text <- function(row_value) {
  temp <- final_index %>%
    slice(row_value)
  
 
  temp_filename <- temp$filename
  
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
 
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

articles_df <- tibble()
row_values <- 1:nrow(final_index)

lapply(row_values, create_article_text)


# Clean up articles_df and join to index dataframe
articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(final_index)

#After viewing articles_df, I see 2022 lines from the index that I don't need. Cutting them 

articles_df <- articles_df %>%
  slice(-c(1:2022)) |> 
  #gets rid of blank rows
    filter(trimws(sentence) != "") 

write_csv(articles_df, "../AYClassProjectRepo/extracted files/articles_df.csv")
```


```{r}
# Load required libraries
library(dplyr)
library(stringr)
library(tidytext)


bigrams <- articles_df %>% 
  select(sentence) %>% 
  mutate(
    sentence = str_squish(sentence),                      # Remove extra spaces
    sentence = tolower(sentence),
    sentence = str_replace_all(sentence, c(
    "copyright" = "",
    "new york times"="",
    "publication"="",
    "www.alt"="",
    "http"=""))) %>% 
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%                 # Filter out stop words
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1) & !is.na(word2))   
    
# Ensure `stop_words` data is available
data("stop_words")

# Define the expanded remove pattern for unwanted terms
remove_pattern <- paste(
  "title|pages|publication date|publication subject|publication type|issn|language of publication: english|",
  "document url|copyright|last updated|database|startofarticle|proquest document id|",
  "classification|https|--|people|alt|language|english|length|words|publication|type|morg|york|times|'new york times'|publication   info|illustration|date|caption|[0-9.]|","new york times","alexandria virgina","ms jean", "identifier/keyword|twitter\\.|rauchway|keynes's|_ftn|enwikipediaorg|","wwwnytimescom|wwwoenbat|wwwpresidencyucsbedu|wwwalt|wwwthemoneyillusioncom|","aaa|predated|a_woman_to_reckon_with_the_vision_and_legacy_of_fran|ab_se|",
  "jcr:fec|ac|___________________|\\bwww\\b|[_]+",sep = ""
)

# Process bigrams
bigrams <- articles_df %>% 
  select(sentence) %>% 
  mutate(
    sentence = str_squish(sentence),                      # Remove extra spaces
    sentence = tolower(sentence),                         # Convert to lowercase
    sentence = str_replace_all(sentence, remove_pattern, ""), # Remove unwanted terms
    sentence = str_replace_all(sentence, "- ", ""),       # Remove trailing hyphens
    sentence = str_replace_all(sentence, "\\b[a-zA-Z]\\b", "") # Remove single characters
  ) %>% 
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%                 # Filter out stop words
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word1 %in% remove_pattern) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1) & !is.na(word2))                   # Filter out NAs

bigrams
```


```{r}
nrow(final_data)
ncol(final_data)
```

#top 20 bigrams
```{r}
top_20_bigrams <- bigrams |> 
   top_n(20) |> 
  mutate(bigram = paste(word1, " ", word2)) |> 
  select(bigram, n)
```


```{r}
ggplot(top_20_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  coord_flip() +  
  labs(title = "Top Two-Word phrases in FramingAI articles",
       caption = "n=500 articles. Graphic by Taufiq Ahmad. 11-24-2024",
       x = "Phrase",
       y = "Count of terms")
```

# Prof. Rob, I'm still working on this assignment. I wanted to submit this as a way of progress while I finalize it. 


```
